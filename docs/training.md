# Model Training Documentation

## ğŸ“– Tá»•ng quan

Há»‡ thá»‘ng sá»­ dá»¥ng 2 mÃ´ hÃ¬nh Gradient Boosting Ä‘á»ƒ dá»± Ä‘oÃ¡n doanh sá»‘:
- **XGBoost** (eXtreme Gradient Boosting)
- **LightGBM** (Light Gradient Boosting Machine)

Cáº£ hai Ä‘á»u thuá»™c há» **ensemble learning** - káº¿t há»£p nhiá»u weak learners (decision trees) thÃ nh má»™t strong learner.

---

## ğŸŒ³ Gradient Boosting lÃ  gÃ¬?

### Ã tÆ°á»Ÿng cá»‘t lÃµi

Thay vÃ¬ train 1 model phá»©c táº¡p, gradient boosting train **nhiá»u models Ä‘Æ¡n giáº£n tuáº§n tá»±**, má»—i model há»c tá»« lá»—i cá»§a model trÆ°á»›c:

```
Step 1: Train Treeâ‚ â†’ Predictionâ‚ â†’ Errorâ‚
Step 2: Train Treeâ‚‚ Ä‘á»ƒ predict Errorâ‚ â†’ Predictionâ‚‚ â†’ Errorâ‚‚
Step 3: Train Treeâ‚ƒ Ä‘á»ƒ predict Errorâ‚‚ â†’ Predictionâ‚ƒ â†’ Errorâ‚ƒ
...
Step N: Final Prediction = Treeâ‚ + Treeâ‚‚ + Treeâ‚ƒ + ... + Treeâ‚™
```

### VÃ­ dá»¥ cá»¥ thá»ƒ

**Má»¥c tiÃªu:** Dá»± Ä‘oÃ¡n sales = 1000

```cháº¡y
Round 1: Treeâ‚ predict 800   â†’ Error = 200
Round 2: Treeâ‚‚ predict 150   â†’ Error = 50
Round 3: Treeâ‚ƒ predict 40    â†’ Error = 10
Round 4: Treeâ‚„ predict 8     â†’ Error = 2
...
Final:   800+150+40+8 = 998  â†’ Very close to 1000!
```

Má»—i tree chá»‰ cáº§n há»c **pháº§n cÃ²n thiáº¿u** (residual), khÃ´ng pháº£i toÃ n bá»™ pattern.

### Learning Rate (Shrinkage)

```python
learning_rate = 0.1  # Shrinkage factor
Final = learning_rate Ã— (Treeâ‚ + Treeâ‚‚ + ... + Treeâ‚™)
```

**Táº¡i sao cáº§n learning rate?**
- Learning rate cao (0.3): Learn nhanh, dá»… overfit
- Learning rate tháº¥p (0.01): Learn cháº­m, stable hÆ¡n, cáº§n nhiá»u trees hÆ¡n

**Trade-off:**
- `learning_rate = 0.3`, `n_estimators = 100` â†’ Fast but risky
- `learning_rate = 0.01`, `n_estimators = 1000` â†’ Slow but robust

---

## ğŸ”¥ XGBoost Deep Dive

### CÃ¡ch hoáº¡t Ä‘á»™ng

**1. Level-wise Tree Growth**

XGBoost grow trees theo **level** (táº§ng):

```
         [Root]           â† Level 0
        /      \
      [A]      [B]        â† Level 1 (grow cáº£ 2 nodes)
     /  \     /  \
   [C] [D] [E]  [F]       â† Level 2 (grow cáº£ 4 nodes)
```

- Grow táº¥t cáº£ nodes cÃ¹ng level trÆ°á»›c khi xuá»‘ng level tiáº¿p theo
- **Æ¯u Ä‘iá»ƒm:** Balanced tree, trÃ¡nh quÃ¡ sÃ¢u
- **NhÆ°á»£c Ä‘iá»ƒm:** KhÃ´ng táº­n dá»¥ng háº¿t potential cá»§a tá»«ng branch

**2. Training Process**

```python
params = {
    "n_estimators": 200,       # Max 200 trees
    "max_depth": 6,            # Max depth per tree
    "learning_rate": 0.1,      # Shrinkage
    "subsample": 0.8,          # 80% samples per tree
    "colsample_bytree": 0.8,   # 80% features per tree
}
```

**Má»—i round (tree):**

```
1. Sample 80% of training data (subsample=0.8)
2. Sample 80% of features (colsample_bytree=0.8)
3. Build tree vá»›i max_depth=6
4. Calculate gradients (errors) tá»« previous predictions
5. Fit tree Ä‘á»ƒ predict gradients
6. Update predictions: pred_new = pred_old + learning_rate Ã— tree_pred
7. Evaluate trÃªn validation set
```

**3. Regularization**

XGBoost cÃ³ nhiá»u cÆ¡ cháº¿ regularization Ä‘á»ƒ trÃ¡nh overfitting:

```python
"gamma": 0.1,          # Min loss reduction Ä‘á»ƒ split node
"reg_alpha": 0.5,      # L1 regularization on weights
"reg_lambda": 1.0,     # L2 regularization on weights
"min_child_weight": 3  # Min sum of weights in child
```

**Gamma:**
- Node chá»‰ split náº¿u loss reduction > gamma
- Gamma cao â†’ Ãt split â†’ Tree Ä‘Æ¡n giáº£n hÆ¡n

**L1/L2 Regularization:**
- Penalty trÃªn leaf weights
- Giáº£m weights â†’ Predictions mÆ°á»£t hÆ¡n â†’ Less overfitting

### Hyperparameters Quan trá»ng

| Parameter | Range | áº¢nh hÆ°á»Ÿng |
|-----------|-------|-----------|
| `n_estimators` | 50-300 | Sá»‘ trees. Nhiá»u = máº¡nh hÆ¡n nhÆ°ng cháº­m + risk overfit |
| `max_depth` | 3-10 | Äá»™ sÃ¢u tree. SÃ¢u = capture complex patterns |
| `learning_rate` | 0.01-0.3 | Tá»‘c Ä‘á»™ há»c. Tháº¥p = stable, cáº§n nhiá»u trees |
| `subsample` | 0.6-1.0 | % samples/tree. <1 = stochastic, reduce overfit |
| `colsample_bytree` | 0.6-1.0 | % features/tree. <1 = diversity giá»¯a trees |
| `gamma` | 0-0.5 | Min loss Ä‘á»ƒ split. Cao = conservative |
| `reg_alpha` | 0-1.0 | L1 regularization. Cao = sparse weights |
| `reg_lambda` | 0-1.0 | L2 regularization. Cao = smooth weights |

### Early Stopping

```python
model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=50,
    verbose=True
)
```

**CÆ¡ cháº¿:**

1. Sau má»—i round, tÃ­nh validation metric (RMSE, MAE, etc.)
2. Track best metric vÃ  best iteration
3. Náº¿u **50 rounds liÃªn tiáº¿p** khÃ´ng cáº£i thiá»‡n â†’ STOP
4. Return model vá»›i weights cá»§a **best iteration**

**Timeline thá»±c táº¿:**

```
Round   Val_RMSE   Best_RMSE   Patience_Counter
------  ---------  ----------  ----------------
1       450.2      450.2       0
20      420.5      420.5       0
50      398.3      398.3       0
100     385.7      385.7       0
142     382.1      382.1       0  â† BEST!
143     382.3      382.1       1
144     382.8      382.1       2
...
192     383.5      382.1       50 â†’ STOP!

â†’ Model sá»­ dá»¥ng weights cá»§a Round 142
```

**Táº¡i sao cáº§n early stopping?**
- âœ… TrÃ¡nh overfit (train quÃ¡ lÃ¢u)
- âœ… Tiáº¿t kiá»‡m thá»i gian
- âœ… Tá»± Ä‘á»™ng tÃ¬m optimal sá»‘ trees

---

## ğŸ’¡ LightGBM Deep Dive

### KhÃ¡c biá»‡t vá»›i XGBoost

**1. Leaf-wise Tree Growth**

LightGBM grow theo **leaf** (lÃ¡), khÃ´ng pháº£i level:

```
XGBoost (level-wise):        LightGBM (leaf-wise):
         [Root]                      [Root]
        /      \                    /      \
      [A]      [B]                [A]      [B]
     /  \     /  \                          \
   [C] [D] [E]  [F]                         [C]
                                              \
                                              [D]
```

- Chá»n leaf cÃ³ **highest loss reduction** Ä‘á»ƒ split
- **Æ¯u Ä‘iá»ƒm:** Hiá»‡u quáº£ hÆ¡n, accuracy cao hÆ¡n vá»›i Ã­t trees hÆ¡n
- **NhÆ°á»£c Ä‘iá»ƒm:** Dá»… overfit náº¿u khÃ´ng regularize

**2. Histogram-based Learning**

Thay vÃ¬ xÃ©t táº¥t cáº£ split points, LightGBM:
- Chia features thÃ nh **bins** (histogram)
- Chá»‰ xÃ©t split táº¡i bin boundaries
- â†’ Nhanh hÆ¡n nhiá»u, Ä‘áº·c biá»‡t vá»›i large datasets

**3. Gradient-based One-Side Sampling (GOSS)**

- Giá»¯ láº¡i samples cÃ³ **large gradients** (learn nhiá»u)
- Random sample má»™t pháº§n samples cÃ³ **small gradients**
- â†’ Giáº£m computation mÃ  khÃ´ng máº¥t nhiá»u information

### Hyperparameters Äáº·c biá»‡t

```python
params = {
    "num_leaves": 31,          # Max sá»‘ lÃ¡ (khÃ´ng pháº£i depth!)
    "min_child_samples": 20,   # Min samples trong 1 leaf
    "max_bin": 255,            # Sá»‘ bins cho histogram
    "boosting_type": "gbdt",   # Gradient Boosting Decision Tree
}
```

**num_leaves:**
- Quan trá»ng nháº¥t cho LightGBM
- `num_leaves = 2^max_depth` (náº¿u balanced tree)
- LightGBM: control báº±ng `num_leaves` thay vÃ¬ `max_depth`

**Relationship:**
```
max_depth = 6  â†’ balanced tree cÃ³ 2^6 = 64 leaves
num_leaves = 31 â†’ actual leaves (cÃ³ thá»ƒ imbalanced)
```

**Rule of thumb:** `num_leaves < 2^max_depth` Ä‘á»ƒ trÃ¡nh overfit

### Training Process

```python
model = lgb.LGBMRegressor(
    num_leaves=50,
    learning_rate=0.1,
    n_estimators=200
)

model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    callbacks=[
        lgb.early_stopping(50),
        lgb.log_evaluation(10)  # Log má»—i 10 rounds
    ]
)
```

**Má»—i round:**

1. Calculate gradients vÃ  hessians
2. Build histogram cho features
3. Find best split cho leaf cÃ³ highest gain
4. Split leaf â†’ 2 child leaves
5. Repeat cho leaf tiáº¿p theo (cho Ä‘áº¿n num_leaves)
6. Update predictions
7. Evaluate validation metric

---

## ğŸ¯ Hyperparameter Tuning vá»›i Optuna

### Táº¡i sao cáº§n Tuning?

Default hyperparameters **KHÃ”NG optimal** cho data cá»¥ thá»ƒ:
- Data khÃ¡c nhau â†’ Best params khÃ¡c nhau
- Trade-offs khÃ¡c nhau (speed vs accuracy)

**Manual tuning:**
- Thá»­ params: `{max_depth: 3, lr: 0.1}` â†’ RMSE = 450
- Thá»­ params: `{max_depth: 5, lr: 0.05}` â†’ RMSE = 420
- Thá»­ params: `{max_depth: 7, lr: 0.1}` â†’ RMSE = 410
- ...

â†’ Máº¥t nhiá»u thá»i gian, khÃ´ng systematic!

### Optuna Bayesian Optimization

**Ã tÆ°á»Ÿng:**
- Learn tá»« trials trÆ°á»›c Ä‘á»ƒ suggest params tá»‘t hÆ¡n cho trial sau
- KhÃ´ng pháº£i thá»­ random nhÆ° Grid Search

**Process:**

```python
def objective(trial):
    # 1. Optuna suggest params tá»« search space
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 50, 300),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "subsample": trial.suggest_float("subsample", 0.6, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
        "gamma": trial.suggest_float("gamma", 0, 0.5),
        "reg_alpha": trial.suggest_float("reg_alpha", 0, 1.0),
        "reg_lambda": trial.suggest_float("reg_lambda", 0, 1.0),
    }
    
    # 2. Train model vá»›i params nÃ y
    model = xgb.XGBRegressor(**params)
    model.fit(X_train, y_train, 
              eval_set=[(X_val, y_val)],
              early_stopping_rounds=50)
    
    # 3. Evaluate trÃªn validation
    y_pred = model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, y_pred))
    
    # 4. Return metric Ä‘á»ƒ minimize
    return rmse

# Run optimization
study = optuna.create_study(
    direction="minimize",                      # Minimize RMSE
    sampler=optuna.samplers.TPESampler(seed=42),  # Bayesian optimization
    pruner=optuna.pruners.MedianPruner()      # Stop bad trials early
)

study.optimize(objective, n_trials=50)
best_params = study.best_params
```

### Optuna Timeline

```
Trial 0:  Paramsâ‚€ (random)     â†’ RMSE = 450.2
Trial 1:  Paramsâ‚ (random)     â†’ RMSE = 435.8
Trial 2:  Paramsâ‚‚ (random)     â†’ RMSE = 442.1
Trial 3:  Paramsâ‚ƒ (bayesian)   â†’ RMSE = 428.5 â† Learn tá»« 0,1,2
Trial 4:  Paramsâ‚„ (bayesian)   â†’ RMSE = 420.3 â† Better!
...
Trial 15: Paramsâ‚â‚… (bayesian)  â†’ RMSE = 405.2 â† Best so far
...
Trial 30: Pruned! (khÃ´ng triá»ƒn vá»ng)
...
Trial 50: Paramsâ‚…â‚€ (bayesian)  â†’ RMSE = 408.1

Best trial: Trial 15 vá»›i RMSE = 405.2
```

### TPE Sampler (Tree-structured Parzen Estimator)

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

1. Chia trials thÃ nh 2 groups:
   - **Good trials:** Top 20% vá»›i RMSE tháº¥p nháº¥t
   - **Bad trials:** CÃ²n láº¡i

2. Model distributions:
   - `P(params | good)`: Distribution cá»§a params trong good trials
   - `P(params | bad)`: Distribution cá»§a params trong bad trials

3. Suggest params má»›i:
   - Chá»n params cÃ³ `P(params | good) / P(params | bad)` cao nháº¥t
   - â†’ Params cÃ³ probability cao á»Ÿ good trials, tháº¥p á»Ÿ bad trials

**VÃ­ dá»¥:**
```
Good trials: learning_rate thÆ°á»ng trong [0.05, 0.15]
Bad trials:  learning_rate thÆ°á»ng trong [0.2, 0.3]
â†’ Suggest learning_rate â‰ˆ 0.1 cho trial tiáº¿p theo
```

### Median Pruner

**Má»¥c Ä‘Ã­ch:** Stop trials khÃ´ng triá»ƒn vá»ng sá»›m Ä‘á»ƒ tiáº¿t kiá»‡m thá»i gian

**Logic:**
```python
# Táº¡i má»—i round (e.g., round 50, 100, 150)
current_metric = validation_rmse_at_round_50
median_metric = median(all_completed_trials_at_round_50)

if current_metric > median_metric:
    â†’ Trial nÃ y Ä‘ang worse than median â†’ PRUNE (stop)!
```

**Timeline:**
```
Trial 5 at round 50:  RMSE = 450
Median at round 50:   RMSE = 420
â†’ 450 > 420 â†’ PRUNE! (khÃ´ng cáº§n train Ä‘áº¿n round 200)
```

---

## ğŸ“Š Evaluation Metrics

### 1. RMSE (Root Mean Squared Error)

```python
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
```

**CÃ´ng thá»©c:**
$$RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_{true,i} - y_{pred,i})^2}$$

**Ã nghÄ©a:**
- Sai sá»‘ trung bÃ¬nh theo Ä‘Æ¡n vá»‹ cá»§a target
- **Penalty lá»›n** cho outlier errors (do bÃ¬nh phÆ°Æ¡ng)

**VÃ­ dá»¥:**
```
y_true = [100, 200, 300]
y_pred = [110, 190, 320]
errors = [10, -10, 20]
squared = [100, 100, 400]
MSE = (100+100+400)/3 = 200
RMSE = âˆš200 = 14.14

â†’ Trung bÃ¬nh sai sá»‘ ~14 units
```

**Khi nÃ o dÃ¹ng:**
- âœ… Muá»‘n penalty outliers nhiá»u hÆ¡n
- âœ… Target cÃ³ outliers cáº§n quan tÃ¢m

### 2. MAE (Mean Absolute Error)

```python
mae = mean_absolute_error(y_true, y_pred)
```

**CÃ´ng thá»©c:**
$$MAE = \frac{1}{n}\sum_{i=1}^{n}|y_{true,i} - y_{pred,i}|$$

**Ã nghÄ©a:**
- Sai sá»‘ tuyá»‡t Ä‘á»‘i trung bÃ¬nh
- **Linear penalty** (khÃ´ng bÃ¬nh phÆ°Æ¡ng)

**So vá»›i RMSE:**
```
Same example:
errors = [10, -10, 20]
absolute = [10, 10, 20]
MAE = (10+10+20)/3 = 13.33

MAE < RMSE â†’ RMSE penalty outliers nhiá»u hÆ¡n
```

**Khi nÃ o dÃ¹ng:**
- âœ… Muá»‘n metric dá»… interpret
- âœ… Outliers khÃ´ng quÃ¡ quan trá»ng

### 3. MAPE (Mean Absolute Percentage Error)

```python
mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
```

**CÃ´ng thá»©c:**
$$MAPE = \frac{100}{n}\sum_{i=1}^{n}\left|\frac{y_{true,i} - y_{pred,i}}{y_{true,i}}\right|$$

**Ã nghÄ©a:**
- Sai sá»‘ tÃ­nh theo **pháº§n trÄƒm** cá»§a y_true
- Scale-independent (so sÃ¡nh Ä‘Æ°á»£c giá»¯a datasets khÃ¡c scale)

**VÃ­ dá»¥:**
```
y_true = [100, 1000]
y_pred = [110, 1100]
errors = [10, 100]

MAE = (10+100)/2 = 55 â†’ KhÃ´ng fair!
MAPE = (10/100 + 100/1000)/2 * 100 = (0.1 + 0.1)/2 * 100 = 10%
â†’ Cáº£ 2 predictions Ä‘á»u sai 10%
```

**âš ï¸ Cáº©n tháº­n:**
- KhÃ´ng dÃ¹ng khi y_true cÃ³ giÃ¡ trá»‹ gáº§n 0 (division by zero)
- Asymmetric: over-prediction Ã­t penalty hÆ¡n under-prediction

### 4. RÂ² (Coefficient of Determination)

```python
r2 = r2_score(y_true, y_pred)
```

**CÃ´ng thá»©c:**
$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}} = 1 - \frac{\sum(y_{true} - y_{pred})^2}{\sum(y_{true} - \bar{y})^2}$$

**Ã nghÄ©a:**
- **Tá»· lá»‡ variance** cá»§a y Ä‘Æ°á»£c model explain
- Range: (-âˆ, 1]
  - RÂ² = 1: Perfect prediction
  - RÂ² = 0: Model = baseline (predict mean)
  - RÂ² < 0: Model worse than baseline!

**VÃ­ dá»¥:**
```
y_true = [100, 200, 300, 400]
mean(y_true) = 250

Baseline (predict mean):
SS_tot = (100-250)Â² + (200-250)Â² + (300-250)Â² + (400-250)Â²
       = 22500 + 2500 + 2500 + 22500 = 50000

Model predictions:
y_pred = [110, 190, 310, 390]
SS_res = (100-110)Â² + (200-190)Â² + (300-310)Â² + (400-390)Â²
       = 100 + 100 + 100 + 100 = 400

RÂ² = 1 - 400/50000 = 1 - 0.008 = 0.992

â†’ Model giáº£i thÃ­ch 99.2% variance!
```

**Khi nÃ o dÃ¹ng:**
- âœ… So sÃ¡nh models (RÂ² cao hÆ¡n = tá»‘t hÆ¡n)
- âœ… Hiá»ƒu model fit data tá»‘t Ä‘áº¿n Ä‘Ã¢u
- âŒ KhÃ´ng dÃ¹ng Ä‘á»ƒ compare cross datasets (scale-dependent)

### So sÃ¡nh Metrics

| Metric | Range | Scale | Outlier Sensitivity | Interpretation |
|--------|-------|-------|---------------------|----------------|
| RMSE | [0, âˆ) | Same as y | High | Average error in y units |
| MAE | [0, âˆ) | Same as y | Low | Average absolute error |
| MAPE | [0, âˆ) | Percentage | Medium | % error relative to y |
| RÂ² | (-âˆ, 1] | Unitless | Medium | % variance explained |

**Trong code:**
```python
def calculate_metrics(y_pred, y_true):
    return {
        "rmse": np.sqrt(mean_squared_error(y_true, y_pred)),
        "mae": mean_absolute_error(y_true, y_pred),
        "mape": np.mean(np.abs((y_true - y_pred) / y_true)) * 100,
        "r2": r2_score(y_true, y_pred)
    }

# VÃ­ dá»¥ output:
# {
#   "rmse": 245.67,    â†’ Sai sá»‘ trung bÃ¬nh ~246 sales
#   "mae": 198.32,     â†’ Absolute error ~198 sales  
#   "mape": 12.5,      â†’ Sai sá»‘ ~12.5%
#   "r2": 0.85         â†’ Explain 85% variance
# }
```

---

## ğŸ”„ Complete Training Flow

### Step-by-step Process

```python
# 1. Load data
sale_processed = pd.read_csv("processed.csv")

# 2. Prepare data (split + feature engineering)
train_df, val_df, test_df = trainer.prepare_data(
    df=sale_processed,
    date_col="date",
    target_col="sales",
    group_cols=["store_id"]
)

# 3. Preprocess features (encode + scale)
X_train, X_val, X_test, y_train, y_val, y_test = \
    trainer.preprocess_features(
        train_df, val_df, test_df,
        target_col="sales",
        exclude_cols=['date']
    )

# 4. Train LightGBM vá»›i Optuna tuning
model_lgb = trainer.train_lightgbm_model(
    X_train=X_train,
    X_val=X_val,
    y_train=y_train,
    y_val=y_val,
    use_optuna=True  # Enable hyperparameter tuning
)

# 5. Train XGBoost vá»›i Optuna tuning
model_xgb = trainer.train_xgboot_model(
    X_train=X_train,
    X_val=X_val,
    y_train=y_train,
    y_val=y_val,
    use_optuna=True
)

# 6. Evaluate trÃªn test set
result_lgb = model_lgb.predict(X_test)
metrics_lgb = trainer.calculate_metrics(y_test, result_lgb)

result_xgb = model_xgb.predict(X_test)
metrics_xgb = trainer.calculate_metrics(y_test, result_xgb)

# 7. Compare results
print(f"LightGBM - RMSE: {metrics_lgb['rmse']:.2f}, RÂ²: {metrics_lgb['r2']:.4f}")
print(f"XGBoost  - RMSE: {metrics_xgb['rmse']:.2f}, RÂ²: {metrics_xgb['r2']:.4f}")
```

### Timeline Chi tiáº¿t

**LightGBM Training:**
```
[Optuna] Starting optimization with 50 trials

Trial 0:
  Params: {num_leaves: 25, learning_rate: 0.15, n_estimators: 120, ...}
  [LightGBM] Training...
  [50]   valid's l2: 0.0450
  [100]  valid's l2: 0.0425
  [120]  valid's l2: 0.0418
  â†’ RMSE = 245.67

Trial 1:
  Params: {num_leaves: 45, learning_rate: 0.08, n_estimators: 180, ...}
  [LightGBM] Training...
  [50]   valid's l2: 0.0435
  [100]  valid's l2: 0.0398
  [142]  valid's l2: 0.0385  â† Best iteration
  [192]  valid's l2: 0.0386  â†’ Early stop
  â†’ RMSE = 238.92

... (48 more trials)

Trial 47:
  Params: {num_leaves: 65, learning_rate: 0.087, n_estimators: 215, ...}
  [LightGBM] Training...
  [142]  valid's l2: 0.0380  â† Best iteration
  â†’ RMSE = 232.18  â† BEST!

Best trial: 47
Best params: {num_leaves: 65, learning_rate: 0.087, ...}
Best RMSE: 232.18

Training final model vá»›i best params...
[LightGBM] Final training complete

Test evaluation:
  RMSE: 235.42
  MAE: 189.23
  MAPE: 11.8%
  RÂ²: 0.867
```

**XGBoost Training:** (TÆ°Æ¡ng tá»±)

---

## ğŸ“ˆ Logs Output Explained

### Optuna Trial Logs

```
[I 2025-12-14 10:30:15,123] Trial 0 finished with value: 245.67 and parameters: {...}
[I 2025-12-14 10:30:18,456] Trial 1 finished with value: 238.92 and parameters: {...}
[I 2025-12-14 10:30:20,234] Trial 2 pruned.
```

- `[I]`: Info log level
- `Trial N`: Trial number (0-indexed)
- `value`: Objective value (RMSE trong case nÃ y)
- `parameters`: Hyperparameters Ä‘Æ°á»£c thá»­
- `pruned`: Trial bá»‹ stop sá»›m vÃ¬ khÃ´ng triá»ƒn vá»ng

### LightGBM Training Logs

```
[LightGBM] [Info] Training until validation scores don't improve for 50 rounds
[50]    valid_0's l2: 0.0423
[100]   valid_0's l2: 0.0398
[150]   valid_0's l2: 0.0385
[200]   valid_0's l2: 0.0386
Early stopping, best iteration is [142]
```

- `[N]`: Boosting round number
- `valid_0's l2`: MSE loss trÃªn validation set
  - l2 = MSE (mean squared error)
  - Giáº£m dáº§n = model Ä‘ang há»c
- `Early stopping`: Triggered vÃ¬ 50 rounds khÃ´ng cáº£i thiá»‡n
- `best iteration [142]`: Round 142 cÃ³ loss tháº¥p nháº¥t

### XGBoost Training Logs

```
[0]     validation_0-rmse:450.23
[50]    validation_0-rmse:398.45
[100]   validation_0-rmse:385.12
[150]   validation_0-rmse:382.67
[200]   validation_0-rmse:383.15
Stopping. Best iteration:
[157]   validation_0-rmse:382.45
```

- `[N]`: Boosting round
- `validation_0-rmse`: RMSE trÃªn validation set
- `Best iteration [157]`: Round cÃ³ RMSE tháº¥p nháº¥t

---

## âš™ï¸ Configuration

```yaml
# config.yaml
training:
  train_size: 0.7        # 70% cho training
  val_size: 0.15         # 15% cho validation
  early_stop: 50         # Stop náº¿u 50 rounds khÃ´ng improve
  optuna_trials: 50      # Sá»‘ trials cho hyperparameter tuning

models:
  xgboost:
    params:              # Default params (náº¿u khÃ´ng dÃ¹ng Optuna)
      n_estimators: 200
      max_depth: 6
      learning_rate: 0.1
      subsample: 0.8
      colsample_bytree: 0.8
      
  lightgbm:
    params:
      num_leaves: 31
      learning_rate: 0.1
      n_estimators: 200
      min_child_samples: 20
```

---

## ğŸ“ Best Practices

### 1. Data Splitting
- âœ… Split theo **thá»i gian** cho time series
- âœ… Train trÃªn data cÅ©, test trÃªn data má»›i
- âŒ KhÃ´ng shuffle time series data

### 2. Validation Set
- âœ… LuÃ´n dÃ¹ng validation set riÃªng cho early stopping
- âœ… Validation set pháº£i representative cho test set
- âŒ KhÃ´ng tune trÃªn test set (data leakage!)

### 3. Hyperparameter Tuning
- âœ… DÃ¹ng Optuna Ä‘á»ƒ tiáº¿t kiá»‡m thá»i gian
- âœ… Set reasonable search ranges
- âœ… Start vá»›i Ã­t trials (20) Ä‘á»ƒ test, tÄƒng dáº§n (50-100)
- âŒ KhÃ´ng set range quÃ¡ rá»™ng (lÃ£ng phÃ­ trials)

### 4. Overfitting Prevention
- âœ… Monitor validation metrics
- âœ… Use early stopping
- âœ… Use regularization (L1/L2, gamma)
- âœ… Reduce max_depth/num_leaves náº¿u tháº¥y overfit
- âš ï¸ Train loss << Val loss = Overfitting signal!

### 5. Model Comparison
- âœ… Compare trÃªn **same test set**
- âœ… Look at multiple metrics (RMSE, MAE, RÂ²)
- âœ… Consider training time vs accuracy trade-off
- âœ… Ensemble thÆ°á»ng tá»‘t hÆ¡n single model

---

## ğŸ› Troubleshooting

### Training quÃ¡ cháº­m

**NguyÃªn nhÃ¢n:**
- QuÃ¡ nhiá»u Optuna trials
- `n_estimators` quÃ¡ lá»›n
- Dataset quÃ¡ lá»›n

**Solutions:**
```python
# Giáº£m trials
optuna_trials: 50 â†’ 20

# Giáº£m n_estimators range
"n_estimators": trial.suggest_int("n_estimators", 50, 150)  # Thay vÃ¬ 300

# Subsample data
X_train_sample = X_train.sample(frac=0.5)  # DÃ¹ng 50% data
```

### Overfitting (Train loss << Val loss)

**NguyÃªn nhÃ¢n:**
- Model quÃ¡ complex
- Regularization yáº¿u
- Train quÃ¡ nhiá»u rounds

**Solutions:**
```python
# TÄƒng regularization
"reg_alpha": trial.suggest_float("reg_alpha", 0.5, 2.0)  # TÄƒng tá»« [0, 1.0]
"reg_lambda": trial.suggest_float("reg_lambda", 1.0, 3.0)

# Giáº£m complexity
"max_depth": trial.suggest_int("max_depth", 3, 6)  # Thay vÃ¬ 3-10
"num_leaves": trial.suggest_int("num_leaves", 20, 50)  # Thay vÃ¬ 20-100

# Aggressive early stopping
early_stopping_rounds = 30  # Thay vÃ¬ 50
```

### Underfitting (Cáº£ train vÃ  val loss Ä‘á»u cao)

**NguyÃªn nhÃ¢n:**
- Model quÃ¡ simple
- Learning rate quÃ¡ tháº¥p
- KhÃ´ng Ä‘á»§ features

**Solutions:**
```python
# TÄƒng complexity
"max_depth": trial.suggest_int("max_depth", 6, 12)
"num_leaves": trial.suggest_int("num_leaves", 50, 150)

# TÄƒng learning rate
"learning_rate": trial.suggest_float("learning_rate", 0.05, 0.3)

# Add more features
# â†’ Check feature engineering pipeline
```

### Val RMSE tá»‘t nhÆ°ng Test RMSE kÃ©m

**NguyÃªn nhÃ¢n:**
- Validation set khÃ´ng representative
- Overfit trÃªn validation set (tune quÃ¡ nhiá»u)

**Solutions:**
- TÄƒng validation set size
- K-fold cross validation
- Re-split data vá»›i seed khÃ¡c

---

## ğŸ“š References

- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [XGBoost Parameters](https://xgboost.readthedocs.io/en/stable/parameter.html)
- [LightGBM Documentation](https://lightgbm.readthedocs.io/)
- [LightGBM Parameters](https://lightgbm.readthedocs.io/en/latest/Parameters.html)
- [Optuna Documentation](https://optuna.readthedocs.io/)
- [Gradient Boosting Explained](https://explained.ai/gradient-boosting/)
